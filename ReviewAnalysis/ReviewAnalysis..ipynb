{
  "metadata": {
    "name": "ReviewAnalysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#1. Count the yearly number of reviews.\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import year, count\n\n# Create a SparkSession\nspark \u003d SparkSession.builder \\\n    .appName(\"YourApp\") \\\n    .getOrCreate()\n\n# Load the review dataset from Hive\ndf \u003d spark.table(\u0027review\u0027)\n\n# Calculate the yearly review count based on the \u0027rev_date\u0027 column\nyearly_review_count \u003d df.withColumn(\"year\", year(df[\"rev_date\"])) \\\n    .groupBy(\"year\") \\\n    .agg(count(\"review_id\").alias(\"review_count\")) \\\n    .orderBy(\"year\", ascending\u003dFalse)  # Sort by year in descending order\n\n# Show the results\n\nz.show(yearly_review_count)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#2. Summarize the count of helpful, funny, and cool reviews each year.\n\n# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import year, sum\n\n# Create SparkSession\nspark \u003d SparkSession.builder \\\n    .appName(\"Summarize Reviews by Year\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Load the Hive table \u0027review\u0027\ndf \u003d spark.table(\u0027review\u0027)\n\n# Extract the year from the \u0027rev_date\u0027 column\ndf \u003d df.withColumn(\"year\", year(df[\"rev_date\"]))\n\n# Group by year and summarize the counts of helpful, funny, and cool reviews\nsummary_by_year \u003d df.groupBy(\"year\") \\\n    .agg(sum(\"rev_useful\").alias(\"total_helpful_reviews\"),\n         sum(\"rev_funny\").alias(\"total_funny_reviews\"),\n         sum(\"rev_cool\").alias(\"total_cool_reviews\")) \\\n    .orderBy(\"year\", ascending\u003dFalse)  # Sort by year in descending order\n\n# Show the results\nz.show(summary_by_year)\n\n# Stop SparkSession\nspark.stop()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n# 3. Create a ranking of users based on their total reviews each year.\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import countDistinct, year, desc, dense_rank\r\nfrom pyspark.sql import Window\r\nfrom pyspark.sql import HiveContext\r\n\r\n# Create a SparkSession\r\nspark \u003d SparkSession.builder \\\r\n    .appName(\"User_Reviews_Ranking\") \\\r\n    .enableHiveSupport() \\\r\n    .getOrCreate()\r\n\r\n# Create a HiveContext\r\nhc \u003d HiveContext(spark.sparkContext)\r\n\r\n# Load the review dataset from Hive\r\ndf \u003d hc.table(\u0027review\u0027)\r\n\r\n# Extract the year from the \u0027rev_date\u0027 column\r\ndf \u003d df.withColumn(\"year\", year(df[\"rev_date\"]))\r\n\r\n# Group by user_id and year, and count the number of reviews\r\nuser_yearly_reviews \u003d df.groupBy(\"rev_user_id\", \"year\") \\\r\n    .agg(countDistinct(\"review_id\").alias(\"total_reviews\"))\r\n\r\n# Define a window specification to rank users within each year based on total reviews\r\nwindow_spec \u003d Window.partitionBy(\"year\").orderBy(desc(\"total_reviews\"))\r\n\r\n# Add a rank column based on total_reviews within each year\r\nuser_ranking \u003d user_yearly_reviews.withColumn(\"rank\", dense_rank().over(window_spec))\r\n\r\n# Select the top reviewer (first row) for each year\r\ntop_reviewers_by_year \u003d user_ranking.filter(\"rank \u003d\u003d 1\")\r\n\r\n# Show the results\r\nz.show(top_reviewers_by_year.orderBy(\"year\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#4. Extract the top 5 common words from reviews.\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, split, lower, regexp_replace\nfrom pyspark.sql.functions import desc\n\n# Create a SparkSession\nspark \u003d SparkSession.builder \\\n    .appName(\"Top_5_Common_Words\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Load the review dataset from Hive\ndf \u003d spark.table(\u0027review\u0027)\n\n# Tokenize the reviews into individual words\nwords_df \u003d df.select(explode(split(lower(regexp_replace(\"rev_text\", \"[^a-zA-Z\\\\s]\", \"\")), \"\\\\s+\")).alias(\"word\"))\n\n# Count the occurrences of each word\nword_count_df \u003d words_df.groupBy(\"word\").count()\n\n# Sort the words by their frequency in descending order and show the top 5\ntop_words_df \u003d word_count_df.orderBy(desc(\"count\")).limit(5)\n\n# Show the top 5 most common words\nz.show(top_words_df)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}