{
  "metadata": {
    "name": "CheckinAnalysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark import HiveContext\nfrom pyspark.sql import SparkSession\nhc \u003d HiveContext(sc)\ndf \u003d hc.table(\u0027checkin\u0027)\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#1. Count the yearly check-in frequency.\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import year, count\n\n# Create a SparkSession\nspark \u003d SparkSession.builder \\\n    .appName(\"Yearly Check-in Frequency\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Load the check-in dataset from the Hive table\ncheckin_df \u003d spark.table(\u0027checkin\u0027)\n\n# Extract the year from the \u0027checkin_dates\u0027 column\ncheckin_df \u003d checkin_df.withColumn(\"year\", year(\"checkin_dates\"))\n\n# Group by year and count the number of check-ins\nyearly_checkin_frequency \u003d checkin_df.groupBy(\"year\").agg(count(\"*\").alias(\"checkin_count\"))\n\n# Show the results\nz.show(yearly_checkin_frequency.orderBy(\"year\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#2. Analyze the check-in frequency for each hour of the day.\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import hour, count\n\n# Create a SparkSession\nspark \u003d SparkSession.builder \\\n    .appName(\"Hourly Check-in Frequency\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Load the check-in dataset from the Hive table\ncheckin_df \u003d spark.table(\u0027checkin\u0027)\n\n# Extract the hour from the \u0027checkin_dates\u0027 column\ncheckin_df \u003d checkin_df.withColumn(\"hour_of_day\", hour(\"checkin_dates\"))\n\n# Group by hour of the day and count the number of check-ins\nhourly_checkin_frequency \u003d checkin_df.groupBy(\"hour_of_day\").agg(count(\"*\").alias(\"checkin_count\"))\n\n# Show the results\nz.show(hourly_checkin_frequency.orderBy(\"hour_of_day\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#3. Identify the cities where check-ins are most frequent.\n\nfrom pyspark import HiveContext\nfrom pyspark.sql.functions import count, col\n\n# Create a HiveContext\nhc \u003d HiveContext(sc)\n\n# Load the \u0027checkin\u0027 dataset\ncheckin_df \u003d hc.table(\"checkin\")\n\n# Load the \u0027business\u0027 dataset\nbusiness_df \u003d hc.table(\"business\")\n\n# Join the two datasets on the \u0027business_id\u0027 column\njoined_df \u003d checkin_df.join(business_df, checkin_df.business_id \u003d\u003d business_df.business_id)\n\n# Group by city and count the number of check-ins\ncity_checkin_frequency \u003d joined_df.groupBy(\"city\").agg(count(\"*\").alias(\"checkin_count\")).orderBy(col(\"checkin_count\").desc())\n\n# Show the top 20 cities with the most frequent check-ins\nz.show(city_checkin_frequency.limit(20))\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#4. Show the top 20 merchants with the most frequent check-ins\n\nfrom pyspark import HiveContext\nfrom pyspark.sql.functions import count, col\n\n# Create a HiveContext\nhc \u003d HiveContext(sc)\n\n# Load the \u0027checkin\u0027 dataset\ncheckin_df \u003d hc.table(\"checkin\")\n\n# Load the \u0027business\u0027 dataset\nbusiness_df \u003d hc.table(\"business\")\n\n# Join the two datasets on the \u0027business_id\u0027 column\njoined_df \u003d checkin_df.join(business_df, checkin_df.business_id \u003d\u003d business_df.business_id)\n\n# Group by merchant name and count the number of check-ins\nmerchant_checkin_frequency \u003d joined_df.groupBy(\"name\").agg(count(\"*\").alias(\"checkin_count\")).orderBy(col(\"checkin_count\").desc())\n\n# Set limit to 20 for displaying top merchants\ntop_merchants \u003d merchant_checkin_frequency.limit(20)\n\n# Show the top 20 merchants with the most frequent check-ins\nz.show(top_merchants)\n\n\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}